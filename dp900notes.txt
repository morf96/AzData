Mindhub: http://marketplace.measureup.com/login
Collection: https://learn.microsoft.com/en-us/users/pvkovicsrkaangla-0792/collections/7qeoue46ny4rpn
Challenge: https://learn.microsoft.com/en-gb/users/cloudskillschallenge/collections/1o3xt3rw1mgp?WT.mc_id=cloudskillschallenge_be6235e5-c168-4993-b1bb-e53bade5ddee
Practice Test: https://learn.microsoft.com/en-gb/certifications/exams/dp-900/practice/assessment?assessmentId=24&assessment-type=practice
Exam page: https://learn.microsoft.com/en-gb/certifications/exams/dp-900/
Labs: https://microsoftlearning.github.io/DP-900T00A-Azure-Data-Fundamentals/

------------------
CORE DATA CONCEPTS
------------------
https://learn.microsoft.com/en-us/training/paths/azure-data-fundamentals-explore-core-data-concepts/?source=docs&ns-enrollment-type=Collection&ns-enrollment-id=7qeoue46ny4rpn
- Identify common data formats
- Options for storing data in files
- Options for storing data in databases
- Characteristics of transactional data processing solutions
- Characteristics of analytical data processing solution
--------------------------------------------------------
Entitites: Data structures in which this data is organized often represents entities (customers, products, sales orders).
Can classify data as structured, semi-structured, or unstructured.
Structured data adheres to a fixed schema: (all of the data has the same fields or properties)
	- tabular: a schema for structured data entities
	- often stored in a database in which multiple tables can reference one another by using key values in a relational model
Semi-structured data allows for variation between entity instances.
	- One common format for semi-structured data is JavaScript Object Notation (JSON)
Unstructured data:
	- documents, images, audio and video data, and binary files might not have a specific structure.
	- The schema of unstructured data is typically defined at query time. This means that data can be loaded onto a data platform in its native format.

SLA:
99.9% Downtime hours per year (8,760 – 8,751.24) = 8.76
99.99% Downtime hours per year (8,760 – 8,759.12) = 0.88
99.999% Downtime hours per year (8,760 – 8,759.91) = 0.09
Total cost of ownership (TCO)
Migrate from physical or virtualized on-premises servers to Azure Virtual Machines: strategy is known as lift and shift.

Understand job responsibilities:
	- Data engineers also work with unstructured data and a wide variety of new data types, such as streaming data:
	  ETL > Extract, Transform, Load:
	  	Extract raw data from a structured or unstructured data pool and migrate it to a staging data repository.
		Transform the data from the source schema to the destination schema.
		Load the transformed data into the data warehouse.

          ELT > An alternative approach is Extract, Load, Transform:
	  	Data is immediately extracted and loaded into a large data repository, such as Azure Cosmos DB or Azure Data Lake Storage.
		Reduces the resource contention on source systems. (disadvantage of the ETL approach is that the transformation stage can take a long time.)
		Data engineers can begin transforming the data as soon as the load is complete.
		The data processing framework a data engineer use to ingest data onto cloud data platforms in Azure.

Cloud technical requirements:
- Maintainability
- HA (high availability)
- Multilingual support

Data stores: (two broad categories)<<=
- File stores
- Databases

FILE STORES:
-----------
Delimited text is a good choice for structured data that needs to be accessed by a wide range of applications and services in a human-readable format.
 Data is often stored in plain text format with specific field delimiters and row terminators. 
 The most common format for delimited data is comma-separated values (CSV) in which fields are separated by commas, and rows are terminated by a carriage return / new line. 
 Optionally, the first line may include the field names. Other common formats include tab-separated values (TSV) and space-delimited (in which tabs or spaces are used to separate fields), 
 and fixed-width data in which each field is allocated a fixed number of characters.

JSON is a ubiquitous format in which a hierarchical document schema is used to define data entities (objects) that have multiple attributes.
 Each attribute might be an object (or a collection of objects); making JSON a flexible format that's good for both structured and semi-structured data.
 Collections are enclosed in square brackets [..]
 Objects are enclosed in braces {..}

XML uses tags enclosed in angle-brackets (<../>) to define elements and attributes, was popular in the 1990s and 2000s.

BLOBs (Binary Large Objects) store as binary data (1's and 0's)
 data as raw binary that must be interpreted by applications and rendered, includes images, video, audio, and application-specific documents,
 human-readable formats are mapped to printable characters (typically through a character encoding scheme such as ASCII or Unicode)

Optimised file formats:
	Human-readable formats for structured and semi-structured data typically not optimized for storage space or processing.
	Common optimized file formats:

	Avro is a row-based format. 
	It was created by Apache. Each record contains a header that describes the structure of the data in the record. 
	This header is stored as JSON. The data is stored as binary information. 
	An application uses the information in the header to parse the binary data and extract the fields it contains. 
	Avro is a good format for compressing data and minimizing storage and network bandwidth requirements.

	ORC (Optimized Row Columnar format) organizes data into columns rather than rows. 
	It was developed by HortonWorks for optimizing read and write operations in Apache Hive 
	(Hive is a data warehouse system that supports fast data summarization and querying over large datasets). 
	An ORC file contains stripes of data. Each stripe holds the data for a column or set of columns. 
	A stripe contains an index into the rows in the stripe, the data for each row, and a footer that holds statistical information (count, sum, max, min, and so on) for each column.

	Parquet is another columnar data format. 
	It was created by Cloudera and Twitter. 
	A Parquet file contains row groups. Data for each column is stored together in the same row group. 
	Each row group contains one or more chunks of data. 
	A Parquet file includes metadata that describes the set of rows found in each chunk. 
	An application can use this metadata to quickly locate the correct chunk for a given set of rows, and retrieve the data in the specified columns for these rows. 
	Parquet specializes in storing and processing nested data types efficiently. It supports very efficient compression and encoding schemes.

DATABASES:
---------
A database is used to define a central system in which data can be stored and queried, dedicated system for managing data records rather than files.
(the file system on which files are stored is a kind of database)
Relational databases:
	Each instance of an entity is assigned a primary key that uniquely identifies it; and these keys are used to reference the entity instance in other tables.
	This use of keys to reference data entities enables a relational database to be normalized, eliminate duplicate data, so that, for example, the details of an individual customer are stored only once; not for each sales order the customer places.
	
Non-relational databases, 4 types:
	Key-value databases in which each record consists of a unique key and an associated value, which can be in any format.
	Document databases, which are a specific form of key-value database in which the value is a JSON document (which the system is optimized to parse and query).
	Column family databases, which store tabular data comprising rows and column-families holds a set of columns that are logically related together.
	Graph databases, which store entities as nodes with links to define relationships between them.

OLTP:
A transactional system records transactions that encapsulate specific events that the organization wants to track.
Transactional systems are often high-volume, sometimes handling many millions of transactions in a single day.
Transactional databases are highly normalized and are optimized for CRUD operations.
OLTP solutions rely on a database system in which data storage is optimized for both read and write operations in order to support transactional workloads 
in which data records are created, retrieved, updated, and deleted (often referred to as CRUD operations).
OLTP systems are typically used to support live applications that process business data - often referred to as line of business (LOB) applications.
OLTP systems enforce transactions that support so-called ACID semantics intended to guarantee data validity despite errors, power failures, and other mishaps:
	Atomicity – each transaction is treated as a single unit, which succeeds completely or fails completely.
	Consistency – transaction can only bring the database from one consistent state to another, "referential integrity" guarantees the primary key–foreign key relationship.
	Isolation – ensures that concurrent execution of transactions leaves the database in the same state that would have been obtained if the transactions were executed sequentially, so effects of an incomplete transaction might not be visible to other transactions.
	Durability – guarantees that once a transaction has been committed, it will remain committed even in the case of a system failure.

OLAP:
An OLAP model is an aggregated type of data storage that is optimized for analytical workloads.
Analytical data processing typically uses read-only (or read-mostly) systems that store vast volumes of historical data or business metrics. 
- Data files may be stored in a central Data lake for common in large-scale data analytical processing scenarios.
- An extract, transform, and load (ETL) process copies data from files and OLTP databases into a Data warehouse that is optimized for read activity. 
	A data warehouse schema is based on fact tables that contain numeric values to analyze (for example, sales amounts), 
	with related dimension tables that represent the entities by which you want to measure them (for example, customer or product)
	The data warehouse schema may require some denormalization of data from an OLTP data source (introducing some duplication to make queries perform faster).
	Data Warehouse is a relational database optimised for read operations.
- Cube: Data in the data warehouse may be aggregated and loaded into an online analytical processing (OLAP) model, or cube.
- Aggregated numeric values (measures) from fact tables are calculated for intersections of dimensions from dimension tables. For example, sales revenue might be totaled by date, customer, and product.
The data in the Data lake, Data warehouse, and Analytical model can be queried to produce reports, visualizations, and dashboards.

Data Services:
-------------
Azure SQL (DBMS)
- Azure SQL Database – a fully managed platform-as-a-service (PaaS) database hosted in Azure, best option for create, read, update, and delete (CRUD) operations.
    core database-level capabilities of on-premises SQL Server, this option for new cloud solutions, or to migrate applications that have minimal instance-level dependencies.
    A SQL Database server is a logical construct that acts as a central administrative point for multiple single or pooled databases, logins, firewall rules, auditing rules, threat detection policies, and failover groups.
    available as a Single Database or an Elastic Pool
    can use the Data Migration Assistant to detect compatibility issues with your databases that can impact database functionality in Azure SQL Database. 

- Azure SQL Managed Instance – a hosted instance of SQL Server with automated maintenance, more flexible configuration than Azure SQL DB, databases are maintained in the same SQL Managed Instance cluster
    on-premises databases can be migrated with minimal code changes by using the Azure Database Migration service: https://learn.microsoft.com/en-us/azure/dms
    can install multiple databases on the same instance
    Managed instances depend on other Azure services such as Azure Storage for backups, Azure Event Hubs for telemetry, Azure Active Directory for authentication, Azure Key Vault for Transparent Data Encryption (TDE) 
    and a couple of Azure platform services that provide security and supportability features. The managed instances make connections to these services.
    Data Migration Assistant (DMA): https://www.microsoft.com/download/details.aspx?id=53595
    support features: linked servers, Service Broker (a message processing system that can be used to distribute work across servers), Database Mail
    near 100% compatibility with SQL Server Enterprise Edition, running on-premises.
    
- Azure SQL VM – maximum configurability with full management responsibility, only option that supports all the SQL Server features in the cloud
    databases can easily be "lift and shift" migrated without change.

- Azure SQL Edge - A SQL engine that is optimized for Internet-of-things (IoT) scenarios that need to work with streaming time-series data


Azure Database for open-source relational databases:
- Azure Database for MySQL - a simple-to-use open-source database management system that is commonly used in Linux, Apache, MySQL, and PHP (LAMP) stack apps.
- Azure Database for MariaDB - offers compatibility with Oracle Database, built-in support for temporal data enabling applications to query data as the data appeared in previous points in time.
- Azure Database for PostgreSQL - a hybrid relational-object database, store data in relational tables or store custom data types, with their own non-relational properties,
                                  another key feature is the ability to store and manipulate geometric data, such as lines, circles, and polygons
  Exercise: Explore Azure relational database services
  https://learn.microsoft.com/en-gb/training/modules/explore-provision-deploy-relational-database-offerings-azure/4-exercise-provision-relational-azure-data-services?pivots=azuresql

Azure Cosmos DB 
- global-scale non-relational (NoSQL) database system that supports multiple application programming interfaces (APIs), 
- store and manage data as JSON documents, key-value pairs, column-families, and graphs.
- aggregate data stored in JSON files for use in analytical reports without additional development effort, the SQL API allows querying the documents by using SQL-like language.
- IoT and telematics: store and access data such as Azure Machine Learning, Azure HDInsight, and Power BI
- Microsoft uses Cosmos DB for its own e-commerce platforms that run as part of Windows Store and Xbox Live.
- The Cosmos DB SDKs can be used to build rich iOS and Android applications

- The Gremlin API is used for graph databases, best option for hierarchical data (charts):
	entities are defined as vertices that form nodes in connected graph
	two kinds of vertex (employee and department)
	Gremlin syntax includes functions to operate on vertices and edges, enabling you to insert, update, delete, and query data in the graph:
		g.addV('employee').property('id', '3').property('firstName', 'Alice')
		g.V('3').addE('reports to').to(g.V('1'))
	
- The MongoDB API stores data in the BSON (Binary JSON) format, can be queried by MongoDB Query Language (MQL):
		db.products.find({id: 123})
	results of this query consist of JSON documents:
		{
  		"id": 123,
   		"name": "Hammer",
   		"price": 2.99
		}

- The Table API is used to retrieve key/value pairs (similar to Azure Table Storage)
			PartitionKey	RowKey	Name		Email
			1				123		Joe Jones	joe@litware.com
			1				124		Samir Nadoy	samir@northwind.com

		Use the Table API through one of the language-specific SDKs to make calls to service endpoints to retrieve data from tables example:
		https://endpoint/Customers(PartitionKey='1',RowKey='124')

- The Cassandra API is used to retrieve tabular data in a column-family storage, (syntax based on SQL):
	SELECT * FROM Employees WHERE ID = 2

- The SQL (Core) API that is optimized to store and process (transform) JSON files.
	A SQL query for an Azure Cosmos DB database containing customer data might look similar to this:
		SELECT *
		FROM customers c
		WHERE c.id = "joe@litware.com"
	The result of this query consists of one or more JSON documents, as shown here:
		{
   		"id": "joe@litware.com",
   		"name": "Joe Jones",
   		"address": {
        		"street": "1 Main St.",
        		"city": "Seattle"
    		}
		}
- The PostgreSQL API is used to 
	PostgreSQL is a relational database management system (RDBMS), define relational tables of data, for example a table of products like this:
		ProductID	ProductName	Price
		123			Hammer		2.99
		162			Screwdriver	3.49
	Query this table to retrieve the name and price of a specific product using SQL like this:
		SELECT ProductName, Price 
		FROM Products
		WHERE ProductID = 123;



Azure Storage
- Blob containers - scalable, cost-effective storage for binary files.
		    Data engineers use Azure Storage to host data lakes - blob storage with a hierarchical namespace that enables files to be organized in folders in a distributed file system.
- File shares - network file shares such as you typically find in corporate networks.
- Tables - key-value storage for applications that need to read and write data values quickly.

Azure Data Factory 
- Example for build extract, transform, load (ETL) solutions that populate analytical data stores with data from transactional systems
- Create a pipeline to process data in response to an event

Azure Synapse Analytics - provides a single service interface for multiple analytical capabilities, including:
- Pipelines - based on the same technology as Azure Data Factory.
- SQL - a highly scalable SQL database engine, optimized for data warehouse workloads.
- Apache Spark - an open-source distributed data processing system that supports multiple programming languages and APIs, including Java, Scala, Python, and SQL.
- Azure Synapse Data Explorer - a high-performance data analytics solution that is optimized for real-time querying of log and telemetry data using Kusto Query Language (KQL).
- Integration with services such as Azure Machine Learning and Microsoft Power BI.
- Create a pipeline to process data in response to an event.
- (Databricks and the) Spark pool in Azure Synapse Analytics run data processing for large amounts of data by using Scala.
  Analytics Spark pools do not directly support graph or relational databases, column-family databases are used to store unstructured, tabular data comprising rows and columns.
- Perform near real-time analytics on the operational data (and realted transactions) stored in Azure Cosmos DB.
- Blob storage and Data Lake Storage can be used to store massive amounts of data and can be mounted in Azure Synapse Analytics.

Azure Databricks
- Azure-integrated version of the popular Databricks platform.
- Databricks (and the Apache Spark pool in Azure Synapse Analytics) run data processing for large amounts of data by using Scala.

Azure HDInsight - Azure-hosted cluster support open-source big data analytics workloads:
- Apache Spark - a distributed data processing system that supports multiple programming languages and APIs, including Java, Scala, Python, and SQL.
- Apache Hadoop - a distributed system that uses MapReduce jobs to process large volumes of data across multiple cluster nodes. MapReduce jobs can be written in Java or abstracted by interfaces such as Apache Hive - a SQL-based API that runs on Hadoop.
- Apache HBase - an open-source system for large-scale NoSQL data storage and querying.
- Apache Kafka - a message broker for data stream processing.

Azure Stream Analytics - used to define streaming jobs
- Applies a query to extract and manipulate data from the input stream, writes the results to an output for analysis or further processing.
- Capture streaming data for ingestion into an analytical data store or for real-time visualization.

Azure Data Explorer 
- standalone service, high-performance querying of log and telemetry data
- perform on-demand analysis of large volumes of data from text logs, websites and IoT devices by using a common querying language for all the data sources

Microsoft Purview 
- enterprise-wide data governance and discoverability
- Create a map for data to track data lineage across multiple data sources and systems.

Microsoft Power BI 
- platform for analytical data modeling and reporting

Azure Time Series Insights: https://insights.timeseries.azure.com/

------------------------
RELATIONAL DATA IN AZURE
------------------------
https://learn.microsoft.com/en-us/training/paths/azure-data-fundamentals-explore-relational-data/?source=docs&ns-enrollment-type=Collection&ns-enrollment-id=7qeoue46ny4rpn
- Identify characteristics of relational data
- Define normalization
- Identify types of SQL statement
- Identify common relational database objects
---------------------------------------------
Relational tables are a format for structured data.
Each row in a table has the same amount of columns.
Example: a table might include a MiddleName column which can be empty (or NULL).
A relational database is the best option for CRUD operations and uses the least amount of storage space.
Datatypes:
Each column stores data of a specific datatype:
	an Email column in a Customer table would likely be defined to store character-based (text) data (which might be fixed or variable in length), 
	a Price column in a Product table might be defined to store decimal numeric data, while 
	a Quantity column in an Order table might be constrained to integer numeric values; and 
	an OrderDate column in the same Order table would be defined to store date/time values. 
	The available datatypes used when defining a table depend on the database system.
	Standard datatypes defined by the American National Standards Institute (ANSI) that are supported by most database systems.
Normalisation:
A term used by database professionals for a schema design process that minimizes data duplication and enforces data integrity.
Process of refactoring data (simple definition):
 - Separate each entity into its own table.
 - Separate each discrete attribute into its own column.
 - Uniquely identify each entity instance (row) using a primary key.
 - Use foreign key columns to link related entities.
 - Composite Keys for Tables: a unique combination of multiple columns with primary or foreign keys.
   For example, the LineItem table in the example above uses a unique combination of OrderNo and ItemNo to identify a line item from an individual order.


Common relational database management systems that use SQL include Microsoft SQL Server, MySQL, PostgreSQL, MariaDB, and Oracle.
SQL was originally standardized by the American National Standards Institute (ANSI) in 1986, and by the International Organization for Standardization (ISO) in 1987.
Database vendors include their own proprietary extensions that are not part of the standard, which has resulted in a variety of dialects of SQL:
 - Azure database services that are based on the SQL Server database engine, use Transact-SQL.
 - pgSQL. This is the dialect, with extensions implemented in PostgreSQL.
 - PL/SQL. This is the dialect used by Oracle. PL/SQL stands for Procedural Language/SQL.

SQL statements are grouped into three main logical groups:
- Data Definition Language (DDL) - create, modify, and remove tables and other objects in a database (table, stored procedures, views)
	CREATE	Create a new object in the database, such as a table or a view.
	ALTER	Modify the structure of an object. For instance, altering a table to add a new column.
	DROP	Remove an object from the database.
	RENAME	Rename an existing object.

	Columns marked as NOT NULL are referred to as mandatory columns.
	INT (an integer, or whole number)
	DECIMAL (a decimal number)
	VARCHAR stands for variable length character data
	NULL value an empty column in a row 

	CREATE TABLE Product
	(
	   ID INT PRIMARY KEY,
 	   Name VARCHAR(20) NOT NULL,
 	   Price DECIMAL NULL
	);

- Data Control Language (DCL) - manage access to objects in a database by granting, denying, or revoking permissions to specific users or groups
	GRANT	Grant permission to perform specific actions
	DENY	Deny permission to perform specific actions
	REVOKE	Remove a previously granted permission
	
	GRANT SELECT, INSERT, UPDATE
	ON Product
	TO user1;
	
- Data Manipulation Language (DML) - retrieve (query) data, insert new rows, or modify existing rows
	SELECT	Read rows from a table, which columns
	INSERT	Insert new rows into a table
	UPDATE	Modify data in existing rows
	DELETE	Delete existing rows
	WHERE clause with these statements above to specify criteria
	      only rows that match these criteria will be selected, updated, or deleted.
	JOIN clause for SELECT statements that retrieve data from multiple tables based on a shared key.
	Aliases: abbreviate the table names

	SELECT FirstName, LastName, Address, City
	FROM Customer
	WHERE City = 'Seattle'
	ORDER BY LastName;

	SELECT o.OrderNo, o.OrderDate, c.Address, c.City
	FROM Order AS o
	JOIN Customer AS c
	ON o.Customer = c.ID

	UPDATE Customer
	SET Address = '123 High St.'
	WHERE ID = 1; (If you omit the WHERE clause, an UPDATE statement will modify every row in the table.)

	DELETE FROM Product
	WHERE ID = 162;	

	INSERT INTO Product(ID, Name, Price)
	VALUES (99, 'Drill', 4.99);

	HAVING is used to filter content from a GROUP BY command. 
	UNION displays the content of two sets of columns from two tables but is not based on a shared key. 
	INTERSECT shows only values that exist in both tables.

Database objects:
 - Table
 - View: virtual table based on the results of a SELECT query
	
	Create a view on the Order and Customer tables that retrieves order and customer data to determine delivery addresses easy:

	CREATE VIEW Deliveries
	AS
	SELECT o.OrderNo, o.OrderDate,
      	       c.FirstName, c.LastName, c.Address, c.City
	FROM Order AS o JOIN Customer AS c
	ON o.Customer = c.ID;
	
	SELECT OrderNo, OrderDate, LastName, Address
	FROM Deliveries
	WHERE City = 'Seattle';

 - Index: specify a column and the index contains a copy of this data with pointers to the corresponding rows in the table
	WHERE clause: the database management system can use this index to fetch the data more quickly than if it had to scan through the entire table row by row.
	For a table containing few rows, using the index is probably not any more efficient than simply reading the entire table and finding the rows requested by the query (in which case the query optimizer will ignore the index).
	create an index on the Name column of the Product table:

	CREATE INDEX idx_ProductName
	ON Product(Name);
	
 - Stored procedure: can be run on command
	change the name of a product based on the specified product ID:
	
	CREATE PROCEDURE RenameProduct
	@ProductID INT,
	@NewName VARCHAR(20)
	AS
	UPDATE Product
	SET Name = @NewName
	WHERE ID = @ProductID;
	
	EXEC RenameProduct 201, 'Spanner';

----------------------------
NON-RELATIONAL DATA IN AZURE
----------------------------
https://learn.microsoft.com/en-us/training/paths/azure-data-fundamentals-explore-non-relational-data/?source=docs&ns-enrollment-type=Collection&ns-enrollment-id=7qeoue46ny4rpn
- Describe features and capabilities of Azure blob storage
- Describe features and capabilities of Azure Data Lake Gen2
- Describe features and capabilities of Azure file storage
- Describe features and capabilities of Azure table storage
- Provision and use an Azure Storage account
--------------------------------------------
Blobs - read and write them by using the Azure blob storage API.
	container provides a way of grouping related blobs together
        within a container, organize blobs in a hierarchy of virtual folders 
        	- using a "/" character in a blob name to organize the blobs into namespaces
        	- can't perform folder-level operations to control access or perform bulk operations
	Azure Files is used to store and share files by using SMB and NFS (enables share up to 100 TB of data in a single storage account)
	Table storage is used to store key/value pairs in partitions. 
			The partition key that identifies the partition containing the row, and a row key that is unique to each row in the same partition.
			Azure Table Storage splits a table into partitions. 
			Partitioning is a mechanism for grouping related rows, based on a common property or partition key. 
			Rows that share the same partition key will be stored together. 
			Partitioning not only helps to organize data, it can also improve scalability and performance in the following ways:
				Partitions are independent from each other, and can grow or shrink as rows are added to, or removed from, a partition. 
				A table can contain any number of partitions.
				When you search for data, you can include the partition key in the search criteria. 
				This helps to narrow down the volume of data to be examined, and improves performance by reducing the amount of I/O 
				(input and output operations, or reads and writes) needed to locate the data.
	Page blobs are used for VHDs.
	Data Lake Storage Gen2 is used to store structured and unstructured data for processing:
		- Systems like Hadoop in Azure HDInsight, Azure Databricks, and Azure Synapse Analytics can mount a distributed file system hosted in Azure Data Lake Store Gen2 and use it to process huge volumes of data.
		- To create an Azure Data Lake Store Gen2 files system, you must enable the Hierarchical Namespace option of an Azure Storage account (can’t revert it to a flat namespace)

-----------------------
DATA ANALYTICS IN AZURE
-----------------------
https://learn.microsoft.com/en-us/training/paths/azure-data-fundamentals-explore-data-warehouse-analytics/?source=docs&ns-enrollment-type=Collection&ns-enrollment-id=7qeoue46ny4rpn
- Identify common elements of a modern data warehousing solution
- Describe key features for data ingestion pipelines
- Identify common types of analytical data store and related Azure services
- Provision Azure Synapse Analytics and use it to ingest, process, and query data
---------------------------------------------------------------------------------
Data warehousing architecture:
	1. Data ingestion and processing 
		– data from one or more transactional data stores, files, real-time streams, or other sources is loaded into a data lake or a relational data warehouse. 
		  The load operation usually involves an extract, transform, and load (ETL) or extract, load, and transform (ELT) process 
		  in which the data is cleaned, filtered, and restructured for analysis. 
		  In ETL processes, the data is transformed before being loaded into an analytical store, while in an ELT process the data is copied to the store and then transformed. 
		  Either way, the resulting data structure is optimized for analytical queries. 
		  The data processing is often performed by distributed systems that can process high volumes of data in parallel using multi-node clusters. 
		  Data ingestion includes both batch processing of static data and real-time processing of streaming data.
	2. Analytical data store 
		– data stores for large scale analytics include relational data warehouses, 
		  file-system based data lakes, 
		  and hybrid architectures that combine features of data warehouses and data lakes (sometimes called data lakehouses or lake databases).
	3. Analytical data model 
		– while data analysts and data scientists can work with the data directly in the analytical data store, 
		  it’s common to create one or more data models that pre-aggregate the data to make it easier to produce reports, dashboards, and interactive visualizations. 
		  Often these data models are described as cubes, 
		  in which numeric data values are aggregated across one or more dimensions (for example, to determine total sales by product and region). 
		  The model encapsulates the relationships between data values and dimensional entities to support "drill-up/drill-down" analysis.
	4. Data visualization 
		– data analysts consume data from analytical models, and directly from analytical stores to create reports, dashboards, and other visualizations. 
		  To show trends, comparisons, and key performance indicators (KPIs) for printed reports, graphs and charts in documents or PowerPoint presentations, web-based dashboards, and interactive environments in which users can explore data visually.
		  		Scatter plots are used to determine a relationship or correlation between two numeric values. 
		  		Pie charts visually compare different values as a proportion of a total. 
		  		Line charts are used to examine trends, usually over time. 
		  		Bar charts are used to compare different values for discrete categories.
Data ingestion pipelines:
	Consist of one or more activities that operate on data
	1. Pipelines use linked services to load and process data (might use an Azure Blob Store linked service to ingest the input dataset)
	2. Create pipelines that orchestrate ETL processes
	4. Use services such as Azure SQL Database to run a stored procedure that looks up related data values, before running a data processing task.
	5. Run a data processing task on Azure Databricks or Azure HDInsight, or apply custom logic using an Azure Function.
	   Pipelines can also include some built-in activities, which don’t require a linked service.
	6. Save the output dataset in a linked service such as Azure Synapse Analytics.
Analytical data stores (two common types, data warehouses, data lakes):
	- Data lakes
	- Data warehouses
	- Hybrid approaches
   Azure services for analytical stores(here are three main services):
		* Azure Synapse Analytics
		* Azure Databricks 
		* Azure HDInsight

	Data Lake is a file store, usually on a distributed file system for high performance data access.
		Schema-on-read approach: (without applying constraints when it's stored)
			Spark or Hadoop are often used to process queries on the stored files and return data for reporting and analytics.
			Supporting a mix of structured, semi-structured, unstructured data analisation without the need for schema enforcement.
	Data Warehouse (relational database):
		Data is stored in a schema that is optimized for data analytics rather than transactional workloads.
		Fact Tables:
			Data from a transactional store is transformed into a schema in which numeric values are stored in central fact tables.
		Dimension Tables:
			Fact tables related to one or more dimension tables that represent entities by which the data can be aggregated.
		Star Schema:
			This kind of fact and dimension table schema is called a star schema.
			Often extended into a snowflake schema by adding additional tables related to the dimension tables to represent dimensional hierarchies, for example, product might be related to product categories. 
			A data warehouse is a great choice when you have transactional data that can be organized into a structured schema of tables, and you want to use SQL to query them.
	Hybrid Approach(Lake Database, Data Lakehouse):
		Combine features of data lakes and data warehouses in a lake database or data lakehouse.
		Data lake has aw data is stored as files with a relational storage layer abstracts the underlying files and expose them as tables, which can be queried using SQL.
		PolyBase:
			SQL pools in Azure Synapse Analytics include PolyBase, which enables you to define external tables based on files in a datalake (and other sources) and query them using SQL. 
		Lake Database approach:
			Separating the storage and compute for a data warehousing solution.
				Use database templates to define the relational schema of the data warehouse.
				While storing the underlying data in data lake storage.
		Data Lakehouse:
			Relatively new approach in Spark-based systems, and are enabled through technologies like Delta Lake 
			Adds relational storage capabilities to Spark, so define tables that enforce schemas and transactional consistency.
			Support batch-loaded and streaming data sources.
			Provide a SQL API for querying.

	Azure services for analytical stores:
	Each of these services can be thought of as an analytical data store.
	They rovide a schema and interface through which the data can be queried.
	In many cases the data is actually stored in a data lake and the service is used to process the data and run queries.
		* Azure Synapse Analytics:
			Native support for log and telemetry analytics with Azure Synapse Data Explorer pools.
			Built in data pipelines for data ingestion and transformation.
			Azure Synapse Studio: interactive user interface www.azuresynapse.net/en-us/home
			https://microsoftlearning.github.io/DP-900T00A-Azure-Data-Fundamentals/Instructions/Labs/dp900-04-synapse-lab.html
			Create interactive notebooks in which Spark code and markdown content can be combined.
			Combine data integrity, reliability of a scalable, high-performance SQL Server based relational data warehouse with the flexibility of a data lake and open-source Apache Spark.
			A Synapse Analytics workspace requires two resource groups in your Azure subscription; one for resources you explicitly create, and another for managed resources used by the service. 
			It also requires an Azure Data Lake Storage Account in which to store data, scripts, and other artifacts.
			SQL is a common language for querying structured datasets, many data analysts find languages like Python useful.
			  Needs to run Python (and other) code in a Spark pool; which uses a distributed data processing engine based on Apache Spark.
		* Azure Databricks:
			Built on Apache Spark, offers native SQL capabilities as well as workload-optimized Spark clusters for data analytics and data science.
			Azure implementation of the Databricks platform.
			Operate in a multi-cloud environment, due to its common use on multiple cloud platforms
		* Azure HDInsight:
			Supports multiple open-source data analytics cluster types.
			Option if your analytics solution relies on multiple open-source frameworks.
			Migrate an existing on-premises Hadoop-based solution to the cloud.
			Not as user-friendly as Azure Synapse Analytics and Azure Databricks.

Batch processing, in which multiple data records are collected and stored before being processed together in a single operation.
Stream processing, in which a source of data is constantly monitored and processed in real time as new data events occur.
	Data scope: 
		Batch processing can process all the data in the dataset. 
		Stream processing typically only has access to the most recent data received, or within a rolling time window (the last 30 seconds, for example).
	Data size: 
		Batch processing is suitable for handling large datasets efficiently. 
		Stream processing is intended for individual records or micro batches consisting of few records.
	Performance: 
		Latency is the time taken for the data to be received and processed. The latency for batch processing is typically a few hours. 
		Stream processing typically occurs immediately, with latency in the order of seconds or milliseconds.
	Analysis: 
		Typically use batch processing to perform complex analytics. 
		Stream processing is used for simple response functions, aggregates, or calculations such as rolling averages.

Combine batch and stream processing to enabling both historical and real-time data analysis (lambda and delta architectures)
	1. Data events from a streaming data source are captured in real-time.
	2. Data from other sources is ingested into a data store (often a data lake) for batch processing.
	3. If real-time analytics is not required, the captured streaming data is written to the data store for subsequent batch processing.
	4. When real-time analytics is required, a stream processing technology is used to prepare the streaming data for real-time analysis or visualization; 
	   often by filtering or aggregating the data over temporal windows.
	5. The non-streaming data is periodically batch processed to prepare it for analysis, and the results are persisted in an analytical data store (often referred to as a data warehouse) for historical analysis.
	6. The results of stream processing may also be persisted in the analytical data store to support historical analysis.
	7. Analytical and visualization tools are used to present and explore the real-time and historical data.


















