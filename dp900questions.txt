https://pts.measureup.com/web/index.php#dashboard.php
------------------------------------------------------------------------------------
Describe core data concepts (31 questions)
------------------------------------------------------------------------------------
1.Data organised in a relational database in Tables containing rows and columns.
Relational databases using multiple tables containing rows with the same set of columns.

2.Azure Cosmosd DB is the most common storage type for semi-structured adat using JSON format.
Curly brackets are used to enclose the document and any sub documents.
In a JSON document each data filed is identified by a label, followed by a colon and the filed value.
{"StreetAddress":"Some City"}

3.Characteristics of semi-structured data:
- Do not need to define a strict schema.
- Can store data that has highly variable structure.
- Allows for variation between instances of data entities.
- Allows for a hierachical schema.
- Data entitites does not have to have the same set of attributes.
- Can use XML or JSON format to store data.
- Can add new attributes to enitites stored in JSON format by adding new keys.

4.Identify data formats for two spreadhseets:
A data enitity models a real-world object, such as a Vehicle in this case, shich has attributes.
The car hire company's data models two entities: vehicle and rental. In case of the vehicle, its attributes are the license plate, make, model, color, year purchased.
The License plate is the key value and unique for every vehicles, referenced in both spreadsheets allowing a single vehicle to have many rentals (one-to-many relationship)
The licenseplate value acts as a foreign key for rentals.
The same information is held for all records in the vehicle and rental spreadsheets and so the data is structure, the tables are related acting as a basic relational database.

5.Attributes represent characteristics of a data entitiy.
For tabular data, the table is the data entity and columns within the table are attributes.
Purchase Date is an attribute of the Purchase entity, and i s a column on the Purchase data table.

6.Records in separate tables are related to each other using keys, which are columns storing unique values per table row.
Tables allow the storage of records in the same structure, containing the same attributes.
Key-value databases are non-relational and can be used to store semi-structured data, allowign variations such as one record may contain a single address as an attribute while other contain three or none at all.
Key-vaule databases allow two components of data to be stored related to an incividual record, a unique key and a value which can contain different types, formats, amounts of data per record.

7.Semi-structured data allows for hierachical schema where data entities set-up in a hierarchical fasion and can be modelled in child/parent relationship.

8.The component of a table used to store unique identifier for intsances of a data entity: Primary Key.
Rows are instances of a data entitiy with a set of values for each column. 
An index is created from several columns to improve the sepeed of queries, and they usually include the primary key.
A Foreign Key is the name given to any column in a database table which references rows in another table by their own unique identifier, allowing to form relationships between tables.

9.Documents in document database - relevant Azure service: Cosmos DB
Each document is identified by a unique key used to identify the document and each document is written or retrieved as a single block.(?!)
The schema is defined internally within the document and each individual document can have a different schema.
This allows for support of denormalised data and variations between enitities.

10.An application that will receive real-time event data from multiple sources which may vary in schema. The appliction should be able to write the data to a data store as quickly as possible.
It should look up the value from the data store by using a unique key.
A key/value data store functions esentially as a large hash table and is optimised for fast data writes.
Each data row is referenced by a single key value.
The pnly operations supported are simple query, insert, delete.
Data updates require the application to rewrite the data for the entire value.
Queries can be run on a key or range of keys.

11.Graph data store made of entities and relationships that are referred as nodes and edges.
There can be multiple relationships between entities including hierachical reltionships to support extensive, complex data.
Makes it easier to perform complex data anaysis and mainly used for social media platforms.

12.Non-relational data descriptions with the appropriate data store:
Object: Large audio and video files that are used as the source for streaming content.
Document: Semi-structured data with each entitiy providing its own field definitions.
Graph: Entities and relationships, including multiple and complaex relationships with other enities.

13.Wrongly recommended data store for text file, videos and images.
A documents data store cannot be used as it stores semi-structured data in fields and values with a flexible structure.
Documents are ususally represented in a JSON format and can have different fields to represent the same class of information.
Should use object data store that is optimised for storing and retreiving large binary objects or blobs such as text files, images, videos.

14.Implement a non-relational data store for a new application.
The application needs to store data in JSON format with a flexibleschema. 
All data enitity details should be stored together in the same structure.
A document data store contains the entire data for an entity stored in single documents with a unique id.

15.A column-family data store consists of row identifiers and a group of information stored in a column. 
Each group of information is stored in independent columns.

16.A graph data store consist of edges and nodes used to store and query complex relationships among entities. 
It stores all the enities as nodes and builds the relationship between those enities by using edges.

17. Document databases and graph databases are examples of non-relational stores.
Documents databases store data in JSON or XML format and do not require all documents to have the same structure. 
Graph databases store information in the form of edges and nodes to present complex relationships such as social interactivity.
The Azure Database for MariaDB and SQL databases are examples of relational databases.
Relational databases store information in the form of tables, which can connect through relationships.
Relational databases are used for highly structured data.

18. In this scenario and application stores user profile data, like shipping addresses and user preferences.
Each user can have multiple addresses and different preference fields.
All user data should be stored and retrieved from a single data source.<<<=
Should use a document database store consisting of entitites that have their related data stored in a single document.
This document data supports flexible shchema and the entity data is usually stored in JSON format. 
They can be used for maintaining the user profile information, making it possible to store all information in a single block.
Should not use a column-family datatabase, grouping related columns into column-families that are used together.
Instead of a Document-based database, can use column-family database to retrieve only the required columns-families instead of all user data.
Should not use Key-value store, non-relational data store is highly optimised or simple data structures. 
A Key-value store associates erach data value with a key that an be used to access data.

19.Parquet contains row groups, each which stores chunks of data. Stroing data in columns rather than in rows, developed by Apache.
Free and open source file format.
Language agnostic.
Column-based format - files are organized by column, rather than by row, which saves storage space and speeds up analytics queries.
Used for analytics (OLAP) use cases, typically in conjunction with traditional OLTP databases.
Highly efficient data compression and decompression.
Supports complex data types and advanced nested data structures.

20.Components of a Graph Database
Graph databases store semi-structured data.
Node: Instances of data entities, like people or companies. (analogous to rows in a table)
Edge: Relationships between instances of data entities like company/employee.
        Edges can be directed or undirected depending on the structure of the database.
Property: Attributes of data entities, like a person's email address. (analogous to columns in a table)
Directions: They are properties of an Edge.
        Edgescan be directed or undericted.
        Directed Edges store two pieces of information relating to each of the nodes they connect.
        For instance, a parent/child directed edge would store which person node represents the parent and which the child.
        Undirected edges link nodes toghether, where the direction of the relationship does not matter; for example, a friendhip between two people.

21.Create a data model for a relational database.
Identify the correct components:
Row: Instances of data enitities like people or companies.
Key: Relationships between instances of data entitieslike company/employee.
Column: Attributes of data entities like a person's e-mail address.
Table: represents the data entity itself.

22.CSV:Structured, XML:Semi-structured, BLOB:Unstructured, JSON:Semi-structured

23.A table references values in the "city" table where the ID column is highlighted.
The values are not the Foreign key. Foreign key columns contain values referencing the Primary key of another table.
The City ID column is a foreign key column as it references the primary key values, but this was not highlighted.

24.Online transactional workloads (OLTP)
data is highly normalised with the schema strongly enforced on write.
The Read and write workload requirements are not balanced.
Changes made are rolled back automatically if a transaction is not completed.

25.OLAP & OLTP
An application to perform data mining on holistic data collected from multiplerelational and non-relational sources is an example of an OLAP workload. 
OLAP applications often manipulate data based on complax queries. Data mining queries are complex multidimansional queries that are designed to discover insights from the data that are not immediately present.
An application to process hundreds of user purchases per minute including updates to inventory on hand, is an example of an OLTP workload.
OLTP applications are optimised for write operations and entering and updating data accross multiple, related tables. 
Practical changes made to data are rolled back automatically if a transaction is not completed so that no transaction is left in a partially completed state.
An application to support warehouse sales and shipping for physical warehouses and multiple international locations is an exampl of an OLTP application.
OLTP transactions can be distributed geographically and supported by one or more relational databases. This scenario requires a solution that supports consistent and reliable data writes.
An application to provide loosely normalised data to support report generation is an example of OLAP workload.
Maintaining live data for transactional processing and a separate copy of historic data for analysis, report generation prevents analytic processing from interfering with the performance during transactional processing.

26.Analytical workloads:
generate complex ad-hoc reports using several aggregations,
perform big data analysis on a NoSQL database.
Will not perform e-commerce transactions.

27.Support a reporting sytem that performs data mining in a large amount of data.
Online analytical processing(OLAP) can be used to organise large datasets and perform complex analytics, like data mining without negatively affecting transactional systems.

28.Keep track of all types of financial transactions.
Online transaction processing (OLTP) used with transactional workloads, day-to-day operations for accounting, finance and other systems requirong consistency of transactions.

29.What a Database Administrator do? DBAs are responsible for maintaining and designing databases,
keep the data-related systems up and running, update database structure, create back-ups, maintain good database performance.

30.What a Data Analyst do?
Create reports and dashboards, identify trends and relationships in data, and maintain models and datasets.

31.What tool a Data Analyst use most of the time? Power BI

In the context of databases, a sequence of database operations that satisfies the ACID properties (which can be perceived as a single logical operation on the data) is called a transaction.
ACID: https://en.wikipedia.org/wiki/ACID
Graph database: https://en.wikipedia.org/wiki/Graph_database
Parquet: https://www.databricks.com/glossary/what-is-parquet

Simple DML commands: CREATE/UPDATE/INSERT/DELETE/MERGE

https://learn.microsoft.com/en-us/azure/architecture/guide/technology-choices/data-store-overview
https://learn.microsoft.com/en-us/azure/architecture/data-guide/big-data/non-relational-data
https://learn.microsoft.com/en-us/training/modules/explore-core-data-concepts/2-data-formats
https://learn.microsoft.com/en-us/training/modules/explore-core-data-concepts/3-file-storage
https://learn.microsoft.com/en-us/training/modules/explore-core-data-concepts/4-databases
https://learn.microsoft.com/en-us/training/modules/explore-core-data-concepts/5-transactional-data-processing
https://en.wikipedia.org/wiki/Relational_database
https://learn.microsoft.com/en-us/sql/relational-databases/indexes/indexes?view=sql-server-ver15
https://learn.microsoft.com/en-us/azure/architecture/guide/technology-choices/data-store-overview
https://learn.microsoft.com/en-us/azure/architecture/data-guide/big-data/non-relational-data
https://learn.microsoft.com/en-us/training/modules/explore-provision-deploy-non-relational-data-services-azure/
https://learn.microsoft.com/en-us/dotnet/architecture/cloud-native/relational-vs-nosql-data
https://learn.microsoft.com/en-us/azure/mariadb/overview
https://www.databricks.com/glossary/what-is-parquet
https://en.wikipedia.org/wiki/Graph_database
https://learn.microsoft.com/en-us/training/modules/explore-relational-data-offerings/2-understand-relational-data
https://learn.microsoft.com/en-us/training/modules/explore-relational-data-offerings/3-normalization
https://learn.microsoft.com/en-us/azure/architecture/data-guide/relational-data/online-transaction-processing

Database Administrator
Data Analyst
Data Scientist
Data Engineer
https://learn.microsoft.com/en-us/training/modules/explore-roles-responsibilities-world-of-data/2-explore-job-roles


------------------------------------------------------------------------------------
Identify considerations for relational data on Azure (30 questions)
------------------------------------------------------------------------------------
1.

2.

3.

4.

5.

6.

7.

8.

9.

10.

11.

12.

13.

14.

15.

16.

17.

18.

19.

20.

21.

22.

23. I have te concept of CosmosDB APIs but will have multiple versions od Azure Database as well?

24.

25.

26.

27.

28.

29.

30.

https://azure.microsoft.com/en-us/resources/cloud-computing-dictionary/what-is-saas/
https://binaryterms.com/relational-data-model.html
https://binaryterms.com/relational-data-model.html
https://dev.mysql.com/doc/refman/8.0/en/select.html
https://learn.microsoft.com/en-us/azure/architecture/guide/technology-choices/data-store-overview
https://learn.microsoft.com/en-us/azure/azure-sql/database/sql-database-paas-overview?view=azuresql
https://learn.microsoft.com/en-us/azure/azure-sql/managed-instance/sql-managed-instance-paas-overview?view=azuresql
https://learn.microsoft.com/en-us/azure/azure-sql/virtual-machines/windows/sql-server-on-azure-vm-iaas-what-is-overview?view=azuresql
https://learn.microsoft.com/en-us/azure/cosmos-db/postgresql/howto-ssl-connection-security
https://learn.microsoft.com/en-us/azure/mariadb/overview
https://learn.microsoft.com/en-us/azure/mysql/select-right-deployment-type
https://learn.microsoft.com/en-us/azure/postgresql/single-server/concepts-azure-ad-authentication
https://learn.microsoft.com/en-us/azure/postgresql/single-server/concepts-ssl-connection-security
https://learn.microsoft.com/en-us/azure/postgresql/single-server/overview
https://learn.microsoft.com/en-us/azure/storage/tables/table-storage-overview
https://learn.microsoft.com/en-us/azure/storage/tables/table-storage-overview
https://learn.microsoft.com/en-us/dotnet/architecture/cloud-native/relational-vs-nosql-data
https://learn.microsoft.com/en-us/office/troubleshoot/access/database-normalization-description
https://learn.microsoft.com/en-us/sql/relational-databases/indexes/clustered-and-nonclustered-indexes-described?view=sql-server-ver15
https://learn.microsoft.com/en-us/sql/relational-databases/indexes/heaps-tables-without-clustered-indexes?view=sql-server-ver15
https://learn.microsoft.com/en-us/sql/relational-databases/indexes/indexes?view=sql-server-ver15
https://learn.microsoft.com/en-us/sql/relational-databases/stored-procedures/stored-procedures-database-engine?view=sql-server-ver15
https://learn.microsoft.com/en-us/sql/relational-databases/tables/primary-and-foreign-key-constraints?view=sql-server-ver15
https://learn.microsoft.com/en-us/sql/relational-databases/tables/primary-and-foreign-key-constraints?view=sql-server-ver15&viewFallbackFrom=sql-server-ver1
https://learn.microsoft.com/en-us/sql/relational-databases/views/views?view=sql-server-ver15
https://learn.microsoft.com/en-us/sql/t-sql/functions/sum-transact-sql?view=sql-server-ver15
https://learn.microsoft.com/en-us/sql/t-sql/queries/select-group-by-transact-sql?view=sql-server-ver15
https://learn.microsoft.com/en-us/sql/t-sql/queries/top-transact-sql?view=sql-server-ver15
https://learn.microsoft.com/en-us/sql/t-sql/queries/update-transact-sql?view=sql-server-ver15
https://learn.microsoft.com/en-us/sql/t-sql/statements/create-table-transact-sql?view=sql-server-ver15
https://learn.microsoft.com/en-us/sql/t-sql/statements/drop-table-transact-sql?view=sql-server-ver15
https://learn.microsoft.com/en-us/sql/t-sql/statements/insert-transact-sql?view=sql-server-ver15
https://learn.microsoft.com/en-us/sql/t-sql/statements/statements?view=sql-server-ver15
https://learn.microsoft.com/en-us/sql/t-sql/statements/truncate-table-transact-sql?view=sql-server-ver15
https://learn.microsoft.com/en-us/training/modules/describe-concepts-of-relational-data/2-explore-characteristics
https://learn.microsoft.com/en-us/training/modules/describe-concepts-of-relational-data/3-explore-structures
https://learn.microsoft.com/en-us/training/modules/explore-core-data-concepts/2-data-formats
https://learn.microsoft.com/en-us/training/modules/explore-core-data-concepts/4-databases
https://learn.microsoft.com/en-us/training/modules/explore-provision-deploy-non-relational-data-services-azure/
https://learn.microsoft.com/en-us/training/modules/explore-provision-deploy-relational-database-offerings-azure/2-azure-sql
https://learn.microsoft.com/en-us/training/modules/explore-provision-deploy-relational-database-offerings-azure/3-azure-database-open-source
https://learn.microsoft.com/en-us/training/modules/explore-relational-data-offerings/4-query-with-sql
https://learn.microsoft.com/en-us/training/modules/explore-relational-data-offerings/5-database-objects
https://learn.microsoft.com/en-us/training/modules/query-relational-data/5-azure-database-for-mysql
https://learn.microsoft.com/en-us/training/paths/azure-data-fundamentals-explore-non-relational-data/
https://www.w3schools.com/sql/sql_ref_keywords.asp

------------------------------------------------------------------------------------
Describe considerations for working with non-relational data on Azure (20 questions)
------------------------------------------------------------------------------------
1. Azure Blob is the only Azure storage option that supports access tiers. The defualt is the Hot tier.
The cool tier is optimised for data that will be stored for at leaset 30 days. The Archive storage for data will be stored at least for 180 days.
Archive tier data requires to be rehydrated to a Hot or a Cool tier for access.
Azure blos supports tww performance tiers. The Standard performance tier uses hard disk based storage media. 
The Permium tier uses solid state drive media. The Standard and the Premium options are also supported for Azure File Storage and Azure SQL Database.
Azure Table and Azure File do not support access tiers. 
Access tiers is a featre supported through Cosmos DB APIs. 
Azure Table and Azure File are distinct storage types and are not implemented though Cosmosd DB. 
Table storage is used for storing structured non-relational data. File storage provides fline storage with shared access similar to file server.

2. Azure File Storage supports direct mounting by Windows, macOS and Linux.
Azure File storage is not recommended storage solution for key/value storage implementation. 
Azure Cosmos DB Core (SQL) API suitable for key/value storage also supported by Azure Table storage and Cosmos Table API.
The large file share and premium file shares support LRS and ZRS only in Azure File Storage.

3.

4.

5. Which two elements compose a key in Azure Table Storage?
- Partition Key
- Row Key
Data stored in Aure Table storage is referred to as rows and columns and it forms a table in which the columns may vary according to each row.
The rows in the Table split into partitions and related rows are grouped based on common property. This common property is called Partition key.
The partition key identifies the partititon inside the Azure table storage and a row key is used to uniquely identify each row in a given partititon.
A Value does not compose a key in Azure table storage, rather represents other properties related to a given key, provides data that is returned when query a key.
Unlike in a relational table each row can have different columns of data. 
= > The Table service does not enforce any schema for tables, so two entities in the same table may have different sets of properties.
System Properties: An entity always has the following system properties:
PartitionKey property
RowKey property
Timestamp property 

6. Azure Table Storage stores semi-structured data into a key/value format. This means it stores data into a rows and columns format but unlike 
relational databases, each row has a key and each column contains entire data value.
Understanding the Table service data model: https://learn.microsoft.com/en-us/rest/api/storageservices/Understanding-the-Table-Service-Data-Model
The storage account must always be specified in the request URI. The base URI for accessing the Table service is as follows:
https://myaccount.table.core.windows.net, Tables, Entities, and Properties: 
Tables store data as collections of entities: 
Entities are similar to rows. An entity has a primary key and a set of properties. 
A property is a name, typed-value pair, similar to a column.

7.

8. Enable hierarchical namespace: To implment folder and directory level storage security.
Enabling hierarchical namespace allows to organise Blob containers in folders and directories allowing to define POSIX-complience permissions and RBAC inside containers.
Access control lists (ACLs) in Azure Data Lake Storage Gen2: https://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-access-control

9.

10. By defualt in a Blob storage, Containers act like folders, and logically group related blob files together.

11. Appropriate Cosmosd DB APIs:
Gremlin API for a new application that analyses detailed relationship information for non-relational data.
Cassandra API for moving column-family format data to the cloud to support an existing application.
Core (SQL) API for moving application data to the cloud that uses semi-structred documents to store dta.
Core (SQL) API for a new application that loads key/value format data from multiple sensors.
The Core (SQL) API is used to store semi-structured data in a document and uses SQL-like query language to manipulate the data stored in the documents.
    This includes applications with key/value data. Table API is also supported, but Core (SQL) API is recommended as it provides improved indexing and richer query options.
Non-relational data and NoSQL: https://learn.microsoft.com/en-us/azure/architecture/data-guide/big-data/non-relational-data
Welcome to Azure Cosmos DB: https://learn.microsoft.com/en-us/azure/cosmos-db/introduction
Work with Azure Cosmos DB: https://learn.microsoft.com/en-us/training/modules/work-with-cosmos-db/
Identify Azure Cosmos DB APIs: https://learn.microsoft.com/en-us/training/modules/explore-non-relational-data-stores-azure/3-cosmos-db-apis

12. Cosmos DB general resource hierarch used by all APIs:
Azure Cosmos DB for NoSQL client library for .NET: https://learn.microsoft.com/en-us/azure/cosmos-db/nosql/quickstart-dotnet?tabs=azure-portal%2Cwindows%2Cpasswordless%2Csign-in-azure-cli
There is no blob in the Azure Cosmos DB storage hierarchy.
A storage blob has its own hierarchy of a storage account, container, blob, and items being stored in the blob. 
Azure Cosmos account > (topmost in the resource hierarchy)
    Datbase >
        Container > (create API specific containrs = tables, collections, graphs)
            Item ()

13. The Cassandra API, Cosmos DB API type for Columnar-data (non-relational.)
        This API is compatible with Apache Cassandra databases, which are column-family databases used to store columnar data consisting of row identifiers and a group of information stored in a column. 
        Each group of information is stored in independent data tructures named keyspcaes.
    The Germlin API, Cosmos DB API type for Graph data (non-relational.)
        This API is compatible with Gremlin, which is a graph database that stores nodes andedges for complex relationships among entities.
    The Core API, Cosmos DB API type for JSON documents (non-relational.)
        A document usually contains all data from an entity and each document can have different fields of data.
    The Table API is for storing key-value data organised as rows and columns.
        This Cosmos DB API type is compatible with Azure table storage.
Choose an API in Azure Cosmos DB: https://learn.microsoft.com/en-us/azure/cosmos-db/choose-api

14.
Supports multi-region writes: Cosmos DB API only.
Supports multi-region read replicas: Cosmos DB Table API and Azure Table Storage.

15. You work for a company that is developing a new massively multiplayer online (MMO) game, which will be launched in 20 countries next year.
The data architects have opted to use Azure Cosmos DB as the game's database tier to support geographies and low latency for read and writes.
Common Azure Cosmos DB use cases: https://learn.microsoft.com/en-us/azure/cosmos-db/use-cases
Azure Cosmos DB is a PaaS applciation need some Database Administration. It is a NoSQL database support semi-structured data that used by online games.
Azure Cosmos DB supports multiple APIs such as Cassandra, Gremlin, Table, ...

16.

17. Azure Cosmos DB allows simpler queries to retrieve data than a relational database.
Denormalisation reduces the number of tables within a data model because all attributes from an entity are stored together rather than in different tables.
With fewer tables data read or retreival queries become less complex due to the reduced tabe joins needed.
Cosmos DB supports denormalisation of data which increases the need for data dullication but also reduces the complexity of the schema.

18.

19. Identify query syntax per Cosmosd DB API type.
Is this MondoDB API, Table API, Cassandra API, Core API, Gremlin API?

:> g.V().hasLabel('person').order().by('firstname',decr)

The query refers to the Gremlin API syntax includes funtions to operate on nodes (instances of data enitities) and edges (relationships between nodes), enabling 
users to navigate around the complex graph structure. The g. in the example query stands for graph. 
The examaple statement will retrieve person vertices in descending order of their first names.

Core API, aslo called SQL API uses SQL syntax with keywords in capital letters, spaces ad quotes for string values:

SELECT FirstName, LastName
FROM PERSON
WERE IsChild = true

The query returns with the First and the Last name columns of all rows on the person table marked as children.

The Table API allows manipulation of keyvalue pair data, example query:
https://<mytableendpoint>/People(PrtititonKey='Harp',RowKey='Walter')

The query retrieve from the Poeple table by filtering on the PartititonKey Harp and the RowKey Walter

Cassandra API similar to SQL syntax.

Mondo DB API uses MQL object-oriented syntax:

db.people.find({"isChile":"true"})

The query retrieves people that are children.

Quickstarts:
Query data in Azure Cosmos DB for NoSQL: https://learn.microsoft.com/en-us/azure/cosmos-db/nosql/tutorial-query
Query Azure Cosmos DB by using the API for Table: https://learn.microsoft.com/en-us/azure/cosmos-db/table/tutorial-query
Query Azure Cosmos DB for Gremlin by using Gremlin: https://learn.microsoft.com/en-us/azure/cosmos-db/gremlin/tutorial-query


20. Implementing an Azure Cosmos DB Data Store will need to use several APIs for each non-realtional data store implementation:
Key/value store: Table API or Core API
Column family database: Cassandra API only
Graph database: Gremlin API only

Key-vaule stores are highly optimised for single lookup scenarios, although Core API implements a document-based databse to store JSON-like semi-structured data.
Using Core API for new applications use a document with and item ID and a single value field to represent a key-value store.

Azure Cosmos DB Cassandra API is suitable to represent weather or time-series activity, in a column family databse.

------------------------------------------------------------------------------------
Describe an analytics workload on Azure (37 questions)
------------------------------------------------------------------------------------
1. Azure Synapse Analytics performs complex queries and aggregations on a large amount of relational data.
Provision Synapse SQL pools to quickly execute complex queries accross multiple computer nodes.
MPP > Massively Parallel Processing architecture.
Azure Synapse Analytics: https://learn.microsoft.com/en-us/azure/synapse-analytics/overview-what-is
Dedicated SQL pool (formerly SQL DW) architecture in Azure Synapse Analytics: https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/massively-parallel-processing-mpp-architecture
Analysis Services tabular data in Power BI Desktop: https://learn.microsoft.com/en-us/power-bi/connect-data/desktop-analysis-services-tabular-data
Azure Data Lake Storage Gen2: https://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-introduction
OLTP: https://learn.microsoft.com/en-us/azure/architecture/data-guide/relational-data/online-transaction-processing

2. Azure HDInsight is a big data processing service which is used to provision and manage a cluster of open-source analytics solutions such as Apache Spark, Hadoop, Kafka.
Azure Databricks is a complete analytics platform for big data processing, streaming, and machine learning, optimised for Cloud services on top of Apache Spark.
Azure Analysis Services is a service used to build multidimentional or tabular models using OLAP queries, can combine data from Azure Synapse Analytics, Azude Data Lake Store, Azure Cosmos DB.
Azure Analysis Services: https://learn.microsoft.com/en-us/azure/analysis-services/analysis-services-overview
Databricks: https://learn.microsoft.com/en-us/azure/databricks/introduction/
Azure HDInsight: https://learn.microsoft.com/en-us/azure/hdinsight/hdinsight-overview

3.Databricks can handle batch and stream processing. Complete platform for Big Data procesing and Machine Learning.
Real-time data processin and event streaming from Azure Event Hubs.
Interactive workspace for exploration, data visualisation, collaboration and can run notebooks in R, Python, Scala, SQL.
Azure Event Hubs: https://learn.microsoft.com/en-us/azure/databricks/structured-streaming/streaming-event-hubs

4.Azure Data Lake store raw data. 
Azure Synapse Analytics used to model and serve data. Can load relational data ingested from Azure Data Factory using Synapse SQL pool.
Azure Synapse Analytics can read unstructured data from Azure Data Lake storage using Polybase. Then combine relational and unstructured data, perfom analytics.
Data Warehouse: https://learn.microsoft.com/en-us/azure/architecture/solution-ideas/articles/enterprise-data-warehouse 

5.Azure Data Lake built on top of Azure Blob Storage.
Azure Data Lake storage enables hierachical namespace compatible with Hadoop Distributed File System(HDFS).
Azure Data Lake storage provides a layer to access Azure Blob Storage data as an HDFS storage including support to organise files in directories and subdirectories.
Systems like Hadoop in Azure HDInsight, Azure Databricks, and Azure Synapse Analytics can mount a distributed file system hosted in Azure Data Lake Store Gen2 and use it to process huge volumes of data.
Azure Data Lake Storage Gen2: https://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-introduction

6. Data Warehouse architectire diagram.
Azure Cosmos DB is not part of the Azure data warehouse infrastructure.
 - Raw data: Structured and unstructured.
 - Ingenst Raw data: Azure Data Factory
 - Store data: Azure Data Lake
 - Prepare and transform data: Azure Databricks
 - Model and serve data: Azure Synapse Analytics
Azure Data Factory provides end-to-end support for ETL operations for data warehouse load.
The consolidated data is stored in Azure Blob though Data Lake storage.
Give access to Azure Databricks for data analysis.
Use a native connector to move data from Databricks to Azure Synapse Analytics (acts as a single hub for structured data).
Data factory: https://learn.microsoft.com/en-us/azure/data-factory/introduction

7.Azure Data Factory can have multiple pipelines not necessarily running sequentially. 
A pipeline is a logical grouping of activities that performs a tsk.
Activities in a pipeline either run sequentially or operate in parallel.
An activity represents a step in a pipeline.
Explore data ingestion pipelines: https://learn.microsoft.com/en-us/training/modules/examine-components-of-modern-data-warehouse/3-data-ingestion-pipelines

8.Determine tools for tasks with both relational and non-realtional data.
Extract Data from external sources: Azure Data Factory
Maintain relational and NoSQL data in a data store: Azure Data Lake Storage
Analyse relational and NoSQL data: Azure Synapse Analytics (analyse high volume of sturctured and unstructured data).

9.In Azure Data Factory a pipeline is a logical grouping of activities that define tasks to perform on data.
A Dataset is a representation that maps the data structure inside a pipeline with an extrenal source of destination.
Azure Data Factory uses datasets to perform tasks defines in a pipeline activities to move or transform data.
A linked service is used to povide the connection between a data store and Azure Data Factory.
A Linked service is associated with a dataset and is used to extract or load data in the pipeline.
Linked services in Azure Data Factory and Azure Synapse Analytics: https://learn.microsoft.com/en-us/azure/data-factory/concepts-linked-services?tabs=data-factory
Datasets in Azure Data Factory and Azure Synapse Analytics: https://learn.microsoft.com/en-us/azure/data-factory/concepts-datasets-linked-services?tabs=data-factory
Pipelines and activities in Azure Data Factory and Azure Synapse Analytics: https://learn.microsoft.com/en-us/azure/data-factory/concepts-pipelines-activities?tabs=data-factory

10.Provision services that are compatible with Apache Spark for data processing tasks.
Azure HDInsight is a bit data processing service that is used to provision and manage a cluster of open-source analytics solutions, such as Apache Spark, Hadoop, Kafka.
Azure Databricks is a complete platform for big data processing, streaming, machine learning built on top of Apache Spark.
Should not use Azure Data Factory that is used to ingenst both relational and non-relational data from multiple sources.
Should not use Azure data Lake storage that is built on top of Azure Blob storage compatible with HDFS file system for anaytics solutions.

11.Azure Data Factory can export and load data from Azure Blob Storage, Azure data Lake Storage, Azure Cosmos DB, Azure Synapse Analytics, or outside: Amazon S3.
Azure Data Factory can run SQL Server Integration Service (SSIS) packages using the Execute SSIS Package activiyt. For using Execute SSIS Package activity  configure Azure-SSIS integration runtime (IR).
Run an SSIS package with the Execute SSIS Package activity in Azure portal: https://learn.microsoft.com/en-us/azure/data-factory/how-to-invoke-ssis-package-ssis-activity?tabs=data-factory
Azure Data Factory and Azure Synapse Analytics connector overview: https://learn.microsoft.com/en-us/azure/data-factory/connector-overview

12.In this scenario an Azure Data Factory pipeline ingests employee data, geocodes each record and outputs the data into a data warehouse.
The pipeline is orchestrating an ETL process, the process called data enrichment as the geocoding is being applied before the data reaches the target warehouse.
This is not an ELT process because manipulation of data happens in the target system where the data loaded into the target system as it was in the source system.
Geocoding the employee records is a pipeline activity.
Extract, transform, and load (ETL): https://learn.microsoft.com/en-us/azure/architecture/data-guide/relational-data/etl

13.In this scenario different workloads need appropriate analytical data store.
Data warehouse: Relational data modeled in a star schema
Data warehouse: Relational data modeled in a snowfloke schema 
Data lake: A series of files containing both structured and unstructured data.
Data Lakehouse: A series of unstructured files which can be queried using SQL.
Data warehousees are relational data stores where the schema optimised for analytics. It is denormalised to allow for simpler and faster queries.
Both star and snowflake schemas have a central fact table and orbiting dimension tables (with hierarchies in case of the snowflake).
Data lakes are file stores for high-performance access for structured, semi-structred, unstructured data and their mix.
Data Lakehouses are hybrid of a data warehouse and Data lake storing files with an abstracted relational storage layer which can be queried by SQL.
Explore analytical data stores: https://learn.microsoft.com/en-us/training/modules/examine-components-of-modern-data-warehouse/4-analytical-data-stores

14.Three components of Azure Synapse Analytics:
 - Azure Data Lake Storage Gen2 (Store scripts, data, items in Azure Synapse Analytics)
 - Pipelines (ingest and transform data within Azure Synapse Analytics)
 - Workspaces (work withing Synapse Studio)
Azure Synapse Analytics is a tool that allows massive and efficient data storage, built-in machine learning, AI to provide data insights.
Exercise: Explore data analytics in Azure with Azure Synapse Analytics: https://learn.microsoft.com/en-us/training/modules/examine-components-of-modern-data-warehouse/5-exercise-azure-synapse

15.In this scenario identify processing type for applications:
Batch:
- Employee payroll processing and generating payroll checks.
- Setting inventory stocking lelevels based on seasonal sales volume.
Stream:
- Reporting the number of users and bandwidth use for an online game.
- Identifying defected manufacturing errors to automatically reject failing parts.
https://learn.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/stream-processing
https://learn.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/batch-processing

16.Both Batch and Stream processing can process data from multiple sources.
Data used for transaction processing that requires immediate, consistent posting: Stram processing.
Process a large quantity of data at once: Batch processing.

17.Cannot use stream data to perform complex analytics.
Stream processing with simple functions, aggregates, calculations with individual sets of data.
Should use batch processing to perform complex anaytics.
Big data architecture style: https://learn.microsoft.com/en-us/azure/architecture/guide/architecture-styles/big-data

18.

19.

20.In this scenario collecting real-time data generated by the anti-cheating platform of an online game and selecting workload or processing to use:
Should use Stream processing in this time-critical operation.
Should not ue Batch processing for time-sensitive worklods.
Should not use OLAP, organise databases and perform complex analysis.
Should not use OLTP, accounting, financial, transactional systems require strong consistency for transactions.
Online analytical processing (OLAP): https://learn.microsoft.com/en-us/azure/architecture/data-guide/relational-data/online-analytical-processing
Online transaction processing (OLTP): https://learn.microsoft.com/en-us/azure/architecture/data-guide/relational-data/online-transaction-processing

21.In this scenario select appropriate method of data processing:
Batch: A finance department needs to reconcile bank transactions against orders once a month.
Batch: 



