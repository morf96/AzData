hhttp://marketplace.measureup.com/login
DP-900: Microsoft Azure Data Fundamentals Certification Practice Test
7d81ea52-b06e-476d-83eb-098167183c1c-3761
Order Number: 4061428
Collection: https://learn.microsoft.com/en-us/users/pvkovicsrkaangla-0792/collections/7qeoue46ny4rpn
Challenge: https://learn.microsoft.com/en-gb/users/cloudskillschallenge/collections/1o3xt3rw1mgp?WT.mc_id=cloudskillschallenge_be6235e5-c168-4993-b1bb-e53bade5ddee

CORE DATA CONCEPTS
https://learn.microsoft.com/en-us/training/paths/azure-data-fundamentals-explore-core-data-concepts/?source=docs&ns-enrollment-type=Collection&ns-enrollment-id=7qeoue46ny4rpn
- Identify common data formats
- Options for storing data in files
- Options for storing data in databases
- Characteristics of transactional data processing solutions
- Characteristics of analytical data processing solution
--------------------------------------------------------
Entitites: Data structures in which this data is organized often represents entities (customers, products, sales orders).
Can classify data as structured, semi-structured, or unstructured.
Structured data adheres to a fixed schema: (all of the data has the same fields or properties)
	- tabular: a schema for structured data entities
	- often stored in a database in which multiple tables can reference one another by using key values in a relational model
Semi-structured data allows for variation between entity instances.
	- One common format for semi-structured data is JavaScript Object Notation (JSON)
Unstructured data:
	- documents, images, audio and video data, and binary files might not have a specific structure.
	- The schema of unstructured data is typically defined at query time. This means that data can be loaded onto a data platform in its native format.

SLA:
99.9% Downtime hours per year (8,760 – 8,751.24) = 8.76
99.99% Downtime hours per year (8,760 – 8,759.12) = 0.88
99.999% Downtime hours per year (8,760 – 8,759.91) = 0.09
Total cost of ownership (TCO)
Migrate from physical or virtualized on-premises servers to Azure Virtual Machines: strategy is known as lift and shift.

Understand job responsibilities:
	- Data engineers also work with unstructured data and a wide variety of new data types, such as streaming data:
	  ETL > Extract, Transform, Load:
	  	Extract raw data from a structured or unstructured data pool and migrate it to a staging data repository.
		Transform the data from the source schema to the destination schema.
		Load the transformed data into the data warehouse.

          ELT > An alternative approach is Extract, Load, Transform:
	  	Data is immediately extracted and loaded into a large data repository, such as Azure Cosmos DB or Azure Data Lake Storage.
		Reduces the resource contention on source systems. (disadvantage of the ETL approach is that the transformation stage can take a long time.)
		Data engineers can begin transforming the data as soon as the load is complete.
		The data processing framework a data engineer use to ingest data onto cloud data platforms in Azure.

Cloud technical requirements:
- Maintainability
- HA (high availability)
- Multilingual support

Data stores: (two broad categories)<<=
- File stores
- Databases
FILE STORES:
-----------
Delimited text is a good choice for structured data that needs to be accessed by a wide range of applications and services in a human-readable format.
 Data is often stored in plain text format with specific field delimiters and row terminators. 
 The most common format for delimited data is comma-separated values (CSV) in which fields are separated by commas, and rows are terminated by a carriage return / new line. 
 Optionally, the first line may include the field names. Other common formats include tab-separated values (TSV) and space-delimited (in which tabs or spaces are used to separate fields), 
 and fixed-width data in which each field is allocated a fixed number of characters.

JSON is a ubiquitous format in which a hierarchical document schema is used to define data entities (objects) that have multiple attributes.
 Each attribute might be an object (or a collection of objects); making JSON a flexible format that's good for both structured and semi-structured data.
 Collections are enclosed in square brackets [..]
 Objects are enclosed in braces {..}

XML uses tags enclosed in angle-brackets (<../>) to define elements and attributes, was popular in the 1990s and 2000s.

BLOBs (Binary Large Objects) store as binary data (1's and 0's)
 data as raw binary that must be interpreted by applications and rendered, includes images, video, audio, and application-specific documents,
 human-readable formats are mapped to printable characters (typically through a character encoding scheme such as ASCII or Unicode)

Optimised file formats:
	Human-readable formats for structured and semi-structured data typically not optimized for storage space or processing.
	Common optimized file formats:

	Avro is a row-based format. 
	It was created by Apache. Each record contains a header that describes the structure of the data in the record. 
	This header is stored as JSON. The data is stored as binary information. 
	An application uses the information in the header to parse the binary data and extract the fields it contains. 
	Avro is a good format for compressing data and minimizing storage and network bandwidth requirements.

	ORC (Optimized Row Columnar format) organizes data into columns rather than rows. 
	It was developed by HortonWorks for optimizing read and write operations in Apache Hive 
	(Hive is a data warehouse system that supports fast data summarization and querying over large datasets). 
	An ORC file contains stripes of data. Each stripe holds the data for a column or set of columns. 
	A stripe contains an index into the rows in the stripe, the data for each row, and a footer that holds statistical information (count, sum, max, min, and so on) for each column.

	Parquet is another columnar data format. 
	It was created by Cloudera and Twitter. 
	A Parquet file contains row groups. Data for each column is stored together in the same row group. 
	Each row group contains one or more chunks of data. 
	A Parquet file includes metadata that describes the set of rows found in each chunk. 
	An application can use this metadata to quickly locate the correct chunk for a given set of rows, and retrieve the data in the specified columns for these rows. 
	Parquet specializes in storing and processing nested data types efficiently. It supports very efficient compression and encoding schemes.

DATABASES:
---------
A database is used to define a central system in which data can be stored and queried, dedicated system for managing data records rather than files.
(the file system on which files are stored is a kind of database)
Relational databases:
	Each instance of an entity is assigned a primary key that uniquely identifies it; and these keys are used to reference the entity instance in other tables.
	This use of keys to reference data entities enables a relational database to be normalized, eliminate duplicate data, so that, for example, the details of an individual customer are stored only once; not for each sales order the customer places.
	
Non-relational databases, 4 types:
	Key-value databases in which each record consists of a unique key and an associated value, which can be in any format.
	Document databases, which are a specific form of key-value database in which the value is a JSON document (which the system is optimized to parse and query).
	Column family databases, which store tabular data comprising rows and column-families holds a set of columns that are logically related together.
	Graph databases, which store entities as nodes with links to define relationships between them.

OLTP:
A transactional system records transactions that encapsulate specific events that the organization wants to track.
Transactional systems are often high-volume, sometimes handling many millions of transactions in a single day.
OLTP solutions rely on a database system in which data storage is optimized for both read and write operations in order to support transactional workloads 
in which data records are created, retrieved, updated, and deleted (often referred to as CRUD operations).
OLTP systems are typically used to support live applications that process business data - often referred to as line of business (LOB) applications.
OLTP systems enforce transactions that support so-called ACID semantics intended to guarantee data validity despite errors, power failures, and other mishaps:
	Atomicity – each transaction is treated as a single unit, which succeeds completely or fails completely.
	Consistency – transaction can only bring the database from one consistent state to another, "referential integrity" guarantees the primary key–foreign key relationship.
	Isolation – ensures that concurrent execution of transactions leaves the database in the same state that would have been obtained if the transactions were executed sequentially, so effects of an incomplete transaction might not be visible to other transactions.
	Durability – guarantees that once a transaction has been committed, it will remain committed even in the case of a system failure.

OLAP:
An OLAP model is an aggregated type of data storage that is optimized for analytical workloads.
Analytical data processing typically uses read-only (or read-mostly) systems that store vast volumes of historical data or business metrics. 
- Data files may be stored in a central Data lake for common in large-scale data analytical processing scenarios.
- An extract, transform, and load (ETL) process copies data from files and OLTP databases into a Data warehouse that is optimized for read activity. 
	A data warehouse schema is based on fact tables that contain numeric values to analyze (for example, sales amounts), 
	with related dimension tables that represent the entities by which you want to measure them (for example, customer or product)
	The data warehouse schema may require some denormalization of data from an OLTP data source (introducing some duplication to make queries perform faster).
	Data Warehouse is a relational database optimised for read operations.
- Cube: Data in the data warehouse may be aggregated and loaded into an online analytical processing (OLAP) model, or cube.
- Aggregated numeric values (measures) from fact tables are calculated for intersections of dimensions from dimension tables. For example, sales revenue might be totaled by date, customer, and product.
The data in the Data lake, Data warehouse, and Analytical model can be queried to produce reports, visualizations, and dashboards.

Data Services:
-------------
Azure SQL
- Azure SQL Database – a fully managed platform-as-a-service (PaaS) database hosted in Azure
- Azure SQL Managed Instance – a hosted instance of SQL Server with automated maintenance, allows more flexible configuration than Azure SQL DB.
- Azure SQL VM – a virtual machine with an installation of SQL Server, allowing maximum configurability with full management responsibility.

Azure Database for open-source relational databases
- Azure Database for MySQL - a simple-to-use open-source database management system that is commonly used in Linux, Apache, MySQL, and PHP (LAMP) stack apps.
- Azure Database for MariaDB - offers compatibility with Oracle Database.
- Azure Database for PostgreSQL - a hybrid relational-object database, store data in relational tables or store custom data types, with their own non-relational properties.

Azure Cosmos DB - global-scale non-relational (NoSQL) database system that supports multiple application programming interfaces (APIs), store and manage data as JSON documents, key-value pairs, column-families, and graphs.

Azure Storage
- Blob containers - scalable, cost-effective storage for binary files.
		    Data engineers use Azure Storage to host data lakes - blob storage with a hierarchical namespace that enables files to be organized in folders in a distributed file system.
- File shares - network file shares such as you typically find in corporate networks.
- Tables - key-value storage for applications that need to read and write data values quickly.

Azure Data Factory - exampleÉ build extract, transform, and load (ETL) solutions that populate analytical data stores with data from transactional systems

Azure Synapse Analytics - provides a single service interface for multiple analytical capabilities, including:
- Pipelines - based on the same technology as Azure Data Factory.
- SQL - a highly scalable SQL database engine, optimized for data warehouse workloads.
- Apache Spark - an open-source distributed data processing system that supports multiple programming languages and APIs, including Java, Scala, Python, and SQL.
- Azure Synapse Data Explorer - a high-performance data analytics solution that is optimized for real-time querying of log and telemetry data using Kusto Query Language (KQL).
Integration with services such as Azure Machine Learning and Microsoft Power BI

Azure Databricks - Azure-integrated version of the popular Databricks platform.

Azure HDInsight - Azure-hosted cluster support open-source big data analytics workloads:
- Apache Spark - a distributed data processing system that supports multiple programming languages and APIs, including Java, Scala, Python, and SQL.
- Apache Hadoop - a distributed system that uses MapReduce jobs to process large volumes of data across multiple cluster nodes. MapReduce jobs can be written in Java or abstracted by interfaces such as Apache Hive - a SQL-based API that runs on Hadoop.
- Apache HBase - an open-source system for large-scale NoSQL data storage and querying.
- Apache Kafka - a message broker for data stream processing.

Azure Stream Analytics 
Applies a query to extract and manipulate data from the input stream, writes the results to an output for analysis or further processing.
Capture streaming data for ingestion into an analytical data store or for real-time visualization.

Azure Data Explorer - standalone service, high-performance querying of log and telemetry data

Microsoft Purview - enterprise-wide data governance and discoverability
Create a map for data to track data lineage across multiple data sources and systems.

Microsoft Power BI - platform for analytical data modeling and reporting


RELATIONAL DATA IN AZURE
https://learn.microsoft.com/en-us/training/paths/azure-data-fundamentals-explore-relational-data/?source=docs&ns-enrollment-type=Collection&ns-enrollment-id=7qeoue46ny4rpn

Relational tables are a format for structured data, and each row in a table has the same columns; though in some cases, not all columns need to have a value.
Example: a customer table might include a MiddleName column which can be empty (or NULL) for rows that represent customers with no middle name or whose middle name is unknown.

Each column stores data of a specific datatype. For example, an Email column in a Customer table would likely be defined to store character-based (text) data (which might be fixed or variable in length), a Price column in a Product table might be defined to store decimal numeric data, while a Quantity column in an Order table might be constrained to integer numeric values; and an OrderDate column in the same Order table would be defined to store date/time values. The available datatypes that you can use when defining a table depend on the database system you are using; though there are standard datatypes defined by the American National Standards Institute (ANSI) that are supported by most database systems.











NON-RELATIONAL DATA IN AZURE
https://learn.microsoft.com/en-us/training/paths/azure-data-fundamentals-explore-non-relational-data/?source=docs&ns-enrollment-type=Collection&ns-enrollment-id=7qeoue46ny4rpn


DATA ANALYTICS IN AZURE
https://learn.microsoft.com/en-us/training/paths/azure-data-fundamentals-explore-data-warehouse-analytics/?source=docs&ns-enrollment-type=Collection&ns-enrollment-id=7qeoue46ny4rpn
